##To install docker
brew install docker

K8s Architecture:
Node - it is a machine physical or virtual machine on which k8s is installed it is a worker machine and this is where 
the containers will be launched
Also known as Minion

Problem - what if node fails ?
Ans - the application on that node also fails
Soln - have multiple Node or Cluster

What is Cluster?
It is a group on Node .. or a set of Node and on each node your application is runing
it also helps in load sharing among different nodes

Problem - who will manage this cluster means who will decide how many node are up and report when a node fails then
how it move workload from one node to other
Soln- Master it is just another node with configuration information and its responsible for orchestration of container on
the worker nodes.

When you install Kubernetes on a System, you are actually installing the following
components. An API Server. An ETCD service. A kubelet service. A Container Runtime,
Controllers and Schedulers.
The API server acts as the front-end for kubernetes. The users, management devices,
Command line interfaces all talk to the API server to interact with the kubernetes
cluster.
Next is the ETCD key store. ETCD is a distributed reliable key-value store used by
kubernetes to store all data used to manage the cluster. Think of it this way, when you
have multiple nodes and multiple masters in your cluster, etcd stores all that
information on all the nodes in the cluster in a distributed manner. ETCD is
responsible for implementing locks within the cluster to ensure there are no conflicts
between the Masters.
The scheduler is responsible for distributing work or containers across multiple
nodes. It looks for newly created containers and assigns them to Nodes.
28
The controllers are the brain behind orchestration. They are responsible for noticing
and responding when nodes, containers or endpoints goes down. The controllers
makes decisions to bring up new containers in such cases.
The container runtime is the underlying software that is used to run containers. In our
case it happens to be Docker.
And finally kubelet is the agent that runs on each node in the cluster. The agent is
responsible for making sure that the containers are running on the nodes as
expected.

Ques - How One become master and other become server?
Ans - To be a Master you need following powers
   kub-apiserver 
   etcd
   controller
   scheduler
   The worker node (or minion) as it is also known, is were the containers are hosted. 
   To be a worker you need
    kubelet - that is responsible for interacting with the master to provide health information of the worker node and carry out
    actions requested by the master on the worker nodes. 
    Controller Runtime - its run docker container here we use DOCKER
    
    
Setup K8s
  brew install kubectl
  Download VirtualBox @ virtualbox.org
  brew cask install minikube
  minikube start
  minikube status
  
  Q: What is Minikube
  
  
  Q:What is POD
  
  in k8s we want do deploy our applications into worker nodes form of container in various nodes but containers cant be deployed directly into 
  worker nodes they need to be encapsulated in k8 objects knows as POD
  POD is smallest instance in k8s
  
  Now Lets suppose you have a single instance of your application running inside docker container encapslated inside POD in a single NODE
  what if Load increase -> Would you add mulitiple instance of application inside same pod
  NO POD follows ONE-ONE relationship which means that ONE POD ->ONE Container -> One instance of application instead you will
  create another pod within same node 
  so 1 Node= M*Pods where M is limit of a maxmium POD a node can handle
     1 Pod = 1 Container
     1 Container = Single Instace of application
     
  what if M reaches maximum limit you will add more nodes
  
  1 Kubernetes Cluster = N Nodes
  
But sometimes you might have a scenario were you have a helper container,
that might be doing some kind of supporting task for our web application such as
processing a user entered data, processing a file uploaded by the user etc. and you
want these helper containers to live along side your application container.


How To Deploy PODS
 kubectl run 
 this command deploy the docker container by creating a POD menas that it first
 create a POD and then deploy docker image from it
 Q.Where it get the image?.
 A.form docker hub where various application images are stored
 ex: kubectl run ngnix --image nginx
 Q.how we know list of pods and its detail
 kubectl get pods
 kubectl describe pods
 kubectl get pods -o wide - describe its IP address assigned to it note that each pods gets its own IP
  
 Q.Expose ngnix App Outside of the Cluster in a URL
 
 Summarize What we learnt:
 
A Pod is a Kubernetes abstraction that represents a group of one or more application containers (such as Docker or rkt) and some shared resources for those containers. Those resources include:
Shared storage, as volumes.
Networking, as a unique cluster IP address.
Information about how to run each container, such as the container image version or specific ports to use.

A Pod always runs on a Node. As discussed earlier, Node is a nothing but a worker machine in Kubernetes and may be either a virtual or a physical machine, depending on the cluster. Each Node is managed by the Master. A Node can have multiple pods, and the Kubernetes master automatically handles scheduling the pods across the Nodes in the cluster. The Master’s automatic scheduling takes into account the available resources on each Node.

Every Kubernetes Node runs at least:

kubelet – responsible for communication between the Kubernetes Master and the Nodes
Container runtime (Docker, rkt)

Here are some useful kubectl commands:

kubectl get– list resources
kubectl describe– show detailed information about a resource
kubectl logs– print the logs from a container in a pod
kubectl exec– execute a command on a container in a pod
kubectl delete pod pod-name


YAML in k8s
apiVersion: v1
Kind: Pod
metadata:
   name: myapp-pod
   labels:
      app: myapp
spec:

apiVersion:- This is the version of the kubernetes API we’re using to create the object. 
Depending on what we are trying to create we must use the RIGHT apiVersion. For now since we are working on
PODs, we will set the apiVersion as v1. Few other possible values for this field are
apps/v1beta1, extensions/v1beta1 etc
=========================================================
Kind- refers to the type of object we are trying to create, which in
this case happens to be a POD. So we will set it as Pod. Some other possible values
here could be ReplicaSet or Deployment or Service,
==========================================================
Metadata:- The metadata is data about the object like its name, labels etc.
As you can see unlike the first two were you specified a string value, this, is in the
form of a dictionary. So everything under metadata is intended to the right a little bit
and so names and labels are children of metadata.
===========================================================
SPEC:- Depends on the object we are going to create, this is were we provide additional information to
kubernetes pertaining to that object.Spec is a dictionary so add a property under it called containers, which is a list
or an array. The reason this property is a list is because the PODs can have multiple
containers within them

Note there are 4 Kind in k8s
Kind              Versions
POD                  v1
SERVICE              v1
REPLICASET           apps/v1
DEPLOYMENT           apps/v1   


You can create yaml online on this cool site https://codebeautify.org/yaml-validator

apiVersion: v1
Kind: Pod
metadata: 
    name: myapp-pod
    labels:
        app: myapp
spec: 
    containers: 
        - name: nginx-container
          image: nginx
          
========================================================================
Controllers are the brain behind Kubernetes.
They are processes that monitor kubernetes objects and respond accordingly
          
Replication Center -> why we need it
Reason 1
Let us assume that we have a single POD running our application. What if for some
reason, our application crashes and the POD fails? Users will no longer be able to
access our application. To prevent users from losing access to our application, we
would like to have more than one instance or POD running at the same time. That
way if one fails we still have our application running on the other one. The replication
controller helps us run multiple instances of a single POD in the kubernetes cluster
thus providing High Availability. 

Reason 2
we need replication controller is to create multiple PODs to share the
load across them.
For example, in this simple scenario we have a single POD serving
a set of users. When the number of users increase we deploy additional POD to
balance the load across the two pods. If the demand further increases and If we
were to run out of resources on the first node, we could deploy additional PODs
across other nodes in the cluster. As you can see, the replication controller spans
across multiple nodes in the cluster. It helps us balance the load across multiple pods
on different nodes as well as scale our application when the demand increases.


For example

apiVersion: v1
Kind: ReplicationController
metadata: 
    name: myapp-rc
    labels:
        app: myapp
        type: front-end
spec:   # spec for Replication Center
    - template: # every template is a new POD
        metadata: 
         name: myapp-pod
         labels:
            app: myapp
            type: front-end
        spec: 
            containers: 
            - name: nginx-container
              image: nginx
       - replicas: 3        
       
       
  ReplicaSet
  
  
  apiVersion: v1
Kind: ReplicaSet
metadata: 
    name: myapp-replicaset
    labels:
        app: myapp
        type: front-end
spec:   # spec for Replica Set
    - template: # every template is a new POD
        metadata: 
         name: myapp-pod
         labels:
            app: myapp
            type: front-end
        spec: 
            containers: 
            - name: nginx-container
              image: nginx
      replicas: 3    
      selector:
        matchLabels:
            type: front-end
            
  Command
  
  kuectl create -f replicaset-definiton.yml
  kubectl get replicaset
  kubectl describe replicaset
  kubectl delete replicaset myapp-replicaset
  kubectl replace -f replicaset-definition.yml
  kubectl scale -replicas=6 -f replicaset-d
  
  
  Note here carefully
    kuectl create -f replicaset-definiton.yml
    it will create n number of pods suppose n= 3
    NAME                     READY   STATUS    RESTARTS   AGE
myapp-replicaset-7pn46   1/1     Running   0          13s
myapp-replicaset-glwxj   1/1     Running   0          2m2s
myapp-replicaset-r745t   1/1     Running   0          2m2s

Q:What if I delete any one pod kubectl delete pod myapp-replicaset-7pn46
A: The replicaset will notice it will terminate this pod but as per specification it will start a
new pod so that n=3 maintained

Q:what if you create a new pod with kubectl create -f pod-def.yml with same label
A: Replica Controller will notice it and check label of pod

         if(label_of_new_pod.Equals(label_of_pod_Replicaset))
            delete_that_new_pod
            else
            create_that_new_pod
            
Deployments:
Let us assume that you have a web server that needs to be deployed in a
production environment. You need not ONE, but many such instances of the web
server running for obvious reasons.
Secondly, when newer versions of application builds become available on the docker
registry, you would like to UPGRADE your docker instances seamlessly.
However, when you upgrade your instances, you do not want to upgrade all of them
at once as we just did. This may impact users accessing our applications, so you may
want to upgrade them one after the other. And that kind of upgrade is known as
Rolling Updates.
Suppose one of the upgrades you performed resulted in an unexpected error and you
are asked to undo the recent update. You would like to be able to rollBACK the
changes that were recently carried out.


Commands to note here:
kubectl create -f deployment-definiton.yml # create deployment 
kubectl get dployments 
kubectl get replicaset
kubectl get pods

Observe the flow and remember Deployment create replicaset which create pods SO Dep->Rep->Pod

apiVersion: v1
kind: Deployment
   metadata: 
       name: myapp-replicaset
       labels:
           app: myapp
           type: front-end
   spec:   # spec for Replica Set
       - template: # every template is a new POD
           metadata: 
            name: myapp-pod
            labels:
               app: myapp
               type: front-end
           spec: 
               containers: 
               - name: nginx-container
                 image: nginx
         replicas: 3    
         selector:
           matchLabels:
               type: front-end
                
To See all Created Object at ONCE(Like history of k8s)
kubectl get all

In practical scenario you would be creating this deployment-definition file

Deloyment Stratergy - Rolling Update
Whenever you create a new deployment or upgrade the images in an existing deployment 
it triggers a Rollout.
A rollout is the process of gradually deploying or upgrading your application containers

There are two types of deployment strategies. Say for example you have 5 replicas of
your web application instance deployed. One way to upgrade these to a newer
version is to destroy all of these and then create newer versions of application
instances. Meaning first, destroy the 5 running instances and then deploy 5 new
instances of the new application version it's called Recreate stratergy

Problem: The problem with this as you can imagine, is that during the period after 
the older versions are down and before any newer version is up, the application is down and inaccessible to users.

The second strategy is were we do not destroy all of them at once. Instead we take
down the older version and bring up a newer version one by one. This way the
application never goes down and the upgrade is seamless. called Rolling Update Stratergy
Command : kubectl rollout status deployment/myapp-deploymnent

RollingUpdate is the default Deployment Strategy

Commands
kubectl rollout status deployment/myapp-replicaset # it show the status of deployment whether it is
completed or still some of them need to be deployed
kubectl rollout history deployment/myapp-replicaset 
======================================================================================
Services:
Kubernetes Services enable communication between various components within and
outside of the application. Kubernetes Services helps us connect applications
together with other applications or users This type of service is known as a
NodePort service because the service listens to a port on the Node and forwards
requests to PODs. 

For example, our application has groups of PODs running various sections, such as a group for serving front-end load to users,
another group running back-end processes, and a third group connecting to an
external data source. It is Services that enable connectivity between these groups of
PODs. Services enable the front-end application to be made available to users, it
helps communication between back-end and front-end PODs, and helps in
establishing connectivity to an external data source. Thus services enable loose
coupling between microservices in our application
User -> Service->POD
======================================================================================

The first one is what we discussed already – NodePort were the service makes an
internal POD accessible on a Port on the Node. 
The second is ClusterIP – and in this case the service creates a virtual IP inside
the cluster to enable communication between different services such as a set of front-end servers 
to a set of backendservers. 
The third type is a LoadBalancer, were it provisions a load balancer for our
service in supported cloud providers. A good example of that would be to distribute
load across different web servers
The kubernetes service is an object just like PODs, Replicaset or Deployments that we worked with before. One of
its use case is to listen to a port on the Node and forward requests on that port to a
port on the POD running the web application

--- 
apiVersion: v1
kind: Service
metadata: 
  name: myapp-service
spec: 
    type: NodePort
    ports:
        - targetPort: 80
          port: 80
          nodePort: 300008
    selector:
        app: myapp
        type: front-end

In the spec section of a service we have type and ports. The type refers to
the type of service we are creating. As discussed before it could be ClusterIP,
NodePort, or LoadBalancer

The first type of port is the targetPort, which we will set to 80. 
The next one is simply port, which is the port on the service object and we will set that to 80 as well. 
The third is NodePort which we will set in bewtween 30000 to 32767 or any number in the valid range

Q: How we Connect Service to POD there could be 100 of POD
A: We will use labels and selector

Commands 
kubectl create -f service-definition.yml
kubectl get services
curl http://192.168.X.X

Basic Of Networking in k8s:
How networking works in k8s is different for minikube and OS so lets take a simple example
Let assume you have a cluster with single Node N1 with ip 192.168.1.2 this Node N contain a pod
P1 and we know a POD have a ccontainer but as we see in docker here IP is not assigned to continers 
but to POD so Let this Node N1 have Pods P1,P2,P3 with following IP 10.244.0.2 10.244.0.4 10.244.0.3
and these POD communicate with each other using the following IP







